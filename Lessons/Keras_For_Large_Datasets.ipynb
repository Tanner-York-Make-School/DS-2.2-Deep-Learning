{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras For Large Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a deep learning model on a large dataset\n",
    "In Keras, using fit() and predict() is fine for smaller datasets which can be loaded into memory but in practice, for most practical-use cases, almost all datasets are large and cannot be loaded into memory at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is an example of a data generator to be used to load a large CSV of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def data_gen(df, batch_size):\n",
    "    while True:\n",
    "        x_batch = np.zeros((batch_size, 3, 224, 224))\n",
    "        y_batch = np.zeros((batch_size, 1))\n",
    "        for j in range(len(df['url']/batch_size)):\n",
    "            b = 0\n",
    "            for m, k in zip(df['url'].values[j*batch_size:(j+1)*batch_size], \\\n",
    "                            df['class'].values[j*batch_size:(j+1)*batch_size]):\n",
    "                x_batch[b] = m\n",
    "                y_batch[b] = k\n",
    "                b += 1\n",
    "            yield (x_batch, y_batch)\n",
    "\n",
    "\n",
    "model.fit_generator(generator=data_gen(df_train, batch_size=batch_size), \\\n",
    "                    steps_per_epoch=len(df_train) // batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    " One of the best ways to improve the performance of a Deep Learning model is to add more data to the training set and data aurgmentaion does this by automatically applying different filters to the dataset you already have which in turn creates more varied data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ImageDataGenerator From Keras\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('data/train', target_size=(150, 150), batch_size=32, class_mode='binary')\n",
    "validation_generator = test_datagen.flow_from_directory('data/validation', target_size=(150, 150), batch_size=32, class_mode='binary')\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=2000, epochs=50, validation_data=validation_generator, validation_steps=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning (TL)\n",
    "In practice a very few people train a Convolution network from scratch (random initialization) because it is rare to get enough dataset. So, using pre-trained network weights as initializations or a fixed feature extractor helps in solving most of the problems in hand <br>\n",
    "\n",
    "Very Deep Networks are expensive to train. The most complex models take weeks to train using hundreds of machines equipped with expensive GPUs <br>\n",
    "\n",
    "Determining the topology/flavor/training method/hyper parameters for deep learning is a black art with not much theory to guide you. <br>\n",
    "\n",
    "So, we need transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "\n",
    "base_model = applications.vgg16.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "i=0\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    i = i+1\n",
    "    print(i,layer.name)\n",
    "\n",
    "\n",
    "x = base_model.output\n",
    "x = Dense(128, activation='sigmoid')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.SGD(lr=0.001, momentum=0.9),metrics=[\"accuracy\"])\n",
    "model.fit_generator( train_generator, steps_per_epoch=100, epochs=10, callbacks = callbacks_list, validation_data = \\\n",
    "                    validation_generator, validation_steps=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
