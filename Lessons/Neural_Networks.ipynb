{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "**Neural Network (NN) -** a computational system inspired by the Structure, Processing Method and Learning Ability similar to our biological brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characteristics of an Artifical Neural Network \n",
    "- A large number of simple, neuron-like processing elements that are connected together\n",
    "- Distrobudted representation of knowledge of those elements and their connections that's acquired by some learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Foundations Neural Networks Are Based On\n",
    "**Perceptron -** anything that takes in multiple inputs and creates one output <br>\n",
    "**Multi-layer Perceptron -** a stack of perceptions that are connected together\n",
    "\n",
    "![Multi-layer Perception](../Images/MLP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN takes several input, processes it through multiple neurons from multiple hidden layers and returns the result using an output layer. This result estimation process is technically known as **“Forward Propagation“**\n",
    "\n",
    "Next, we compare the result with actual output. The task is to make the output to neural network as close to actual (desired) output. This defines our cost function.\n",
    "\n",
    "We try to obtain the weight of neurons such that the NN total error (our cost function) being minimized. This process is known as **“Backward Propagation“**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Implementing NN using Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume, we want to build and train (obtain the weights) of a MLP such that for the given input: X=np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]]) that gives us the desired output: y=np.array([[1],[1],[0]]) and only has one hidden layer with three neurons and activation function for each perceptron is sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.97935291]\n",
      " [0.96945842]\n",
      " [0.04508229]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# check this out:\n",
    "# https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/\n",
    "# Input array\n",
    "X=np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
    "\n",
    "#Output\n",
    "y=np.array([[1],[1],[0]])\n",
    "\n",
    "\n",
    "#Sigmoid Function\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "#Derivative of Sigmoid Function\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "# Variables\n",
    "epoch = 5000\n",
    "learning_rate = 0.1\n",
    "input_layer_neurons = X.shape[1]\n",
    "hidden_layer_neurons = 3\n",
    "output_neurons = 1\n",
    "\n",
    "# Wieghts and Biases\n",
    "wh=np.random.uniform(size=(input_layer_neurons,hidden_layer_neurons))\n",
    "bh=np.random.uniform(size=(1,hidden_layer_neurons))\n",
    "wout=np.random.uniform(size=(hidden_layer_neurons,output_neurons))\n",
    "bout=np.random.uniform(size=(1,output_neurons))\n",
    "\n",
    "\n",
    "for i in range(epoch):\n",
    "    #Forward Propogation\n",
    "    hidden_layer_input1=np.dot(X,wh)\n",
    "    hidden_layer_input=hidden_layer_input1 + bh\n",
    "    hiddenlayer_activations = sigmoid(hidden_layer_input)\n",
    "    output_layer_input1=np.dot(hiddenlayer_activations,wout)\n",
    "    output_layer_input= output_layer_input1+ bout\n",
    "    output = sigmoid(output_layer_input)\n",
    "\n",
    "    #Backpropagation\n",
    "    D = y-output\n",
    "    slope_output_layer = derivatives_sigmoid(output)\n",
    "    slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)\n",
    "    d_output = D * slope_output_layer\n",
    "    Error_at_hidden_layer = d_output.dot(wout.T)\n",
    "    d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
    "    wout += hiddenlayer_activations.T.dot(d_output) *learning_rate\n",
    "    bout += np.sum(d_output, axis=0,keepdims=True) *learning_rate\n",
    "    wh += X.T.dot(d_hiddenlayer) *learning_rate\n",
    "    bh += np.sum(d_hiddenlayer, axis=0,keepdims=True) *learning_rate\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
